# -*- coding: utf-8 -*-
"""RAG_Airbnb_Padova.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fSj8qpLtroayIUpilymFsIG08yc6P5ZR
"""

!pip install langchain
!pip install langchain-community
!pip install sentence-transformers      # embedder
!pip install faiss-cpu                  # vector store

!pip install transformers
!pip install bitsandbytes               # model quantization
!pip install torch
!pip install huggingface-hub -q

pip install "accelerate>=0.26.0"

pip install deep-translator

import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import pandas as pd
import numpy as np

# Download required NLTK resources
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')

"""## Dataset construction"""

airbnb1 = pd.read_csv("airbnb1.csv")
airbnb2 = pd.read_csv("airbnb2.csv")
airbnb3 = pd.read_csv("airbnb3.csv")
airbnb4 = pd.read_csv("airbnb4.csv")
airbnb5 = pd.read_csv("airbnb5.csv")
airbnb6 = pd.read_csv("airbnb6.csv")

#Concatenating csv files in a final dataset
df = pd.concat([airbnb1, airbnb2, airbnb3, airbnb4, airbnb5, airbnb6], ignore_index = True)

#Selecting useful information and rename columns
df = df[['id','language','localizedText','localizedReview/localizedDate','reviewer/firstName', 'reviewer/id','startUrl']]
df = df.rename(columns={'localizedText': 'ItalianReview', 'reviewer/firstName':'reviewer_name','reviewer/id':'reviewer_id','startUrl':'url',
                        'localizedReview/localizedDate':'Date'})

#English reviews to be joined in the original dataset
english_reviews1= pd.read_csv("english_reviews1.csv")
english_reviews2= pd.read_csv("english_reviews2.csv")
english_reviews3= pd.read_csv("english_reviews3.csv")

english_data = pd.concat([english_reviews1, english_reviews2, english_reviews3], ignore_index = True)
english_data = english_data.rename(columns={'localizedText':'EnglishReview'})

#Combining data
data = pd.merge(df, english_data, on='id')

print(f"The dataset contains {sum(data.duplicated(subset=['EnglishReview', 'reviewer_name','reviewer_id']))} duplicates")

#Dropping duplicates
data = data.drop_duplicates(subset= ['EnglishReview', 'reviewer_name','reviewer_id'])

print(f"The dataset after dropping contains {sum(data.duplicated(subset=['EnglishReview', 'reviewer_name','reviewer_id']))} duplicates")

#Check for NA values in reviews
print(data['EnglishReview'].isnull().sum())

#Filling NA values for Dates and Years on AirBnB
data['Date'] = data['Date'].fillna('')

def transform_url(url: str) -> str:
    base = url.split('?')[0]  # remove everything after '?'
    return base.replace("www.airbnb.it", "www.airbnb.com")

data["url"] = data["url"].apply(transform_url)

data.head()

add_info1 = pd.read_csv("details1.csv")
add_info2 = pd.read_csv("details2.csv")

add_info1 = add_info1.iloc[:35,:]
add_info2 = add_info2.iloc[:81,:]


add_info = pd.concat([add_info1, add_info2], ignore_index = True)
add_info = add_info.drop_duplicates()
add_info.rename(columns={'address':'city'}, inplace=True)

add_info = add_info.loc[:,['name','additionalHouseRules','city','bathroomLabel','bedroomLabel','guestControls/allowsChildren','guestControls/allowsEvents',
                           'guestControls/allowsInfants','guestControls/allowsPets','guestControls/allowsSmoking','guestControls/personCapacity',
                           'guestControls/structuredHouseRules/1', 'guestControls/structuredHouseRules/0',
                           'guestControls/structuredHouseRules/2','listingExpectations/0/title','listingExpectations/1/title','isHostedBySuperhost',
                           'roomType','url']]
add_info.rename(columns={'guestControls/allowsChildren': 'allowChildren', 'guestControls/allowsEvents':'allowsEvents','guestControls/allowsInfants':'allowsInfant',
                        'guestControls/allowsPets':'allowsPets','guestControls/allowsSmoking':'allowsSmoking','guestControls/personCapacity':'personCapacity',
                         'additionalHouseRules':'HouseRules1','guestControls/structuredHouseRules/0':'HouseRules2','guestControls/structuredHouseRules/1':'HouseRules3',
                         'guestControls/structuredHouseRules/2':'HouseRules4','listingExpectations/0/title':'HouseRules5',
                         'listingExpectations/1/title':'HouseRules6'}, inplace=True)

from deep_translator import GoogleTranslator

# Initialize the translator
translator = GoogleTranslator(source='auto', target='en')
# List of columns to translate
columns_to_translate = ['bathroomLabel', 'bedroomLabel','city','roomType']

# Apply the translation
for col in columns_to_translate:
    add_info[col] = add_info[col].apply(lambda x: translator.translate(x))

add_info.head()

def create_info_dataset(data):
  final_data = pd.DataFrame()
  house_rules_columns = [col for col in data.columns if col.startswith('HouseRules')]
  for col in house_rules_columns:
    data_copy = data.copy()
    data_copy.rename(columns={col: 'Documents'}, inplace=True)
    data_copy.drop(columns=[col for col in data_copy.columns if col.startswith('HouseRules')], inplace=True)
    final_data = pd.concat([final_data,data_copy], ignore_index=True)
  final_data.dropna(subset=['Documents'], inplace=True)

  translator = GoogleTranslator(source='auto', target='en')

  # Translate the 'Text' column
  final_data['Documents'] = final_data['Documents'].apply(lambda x: translator.translate(x))

  return final_data

add_info_final = create_info_dataset(add_info)

info_accomodation = add_info.copy()
info_accomodation = info_accomodation.drop(columns=['HouseRules1','HouseRules2','HouseRules3','HouseRules4','HouseRules5','HouseRules6'])

dataset = pd.merge(data, info_accomodation, on="url")
dataset.drop(columns=["id","language","ItalianReview","reviewer_name","reviewer_id","Date"], inplace=True)
dataset.rename(columns={'EnglishReview':'Documents'}, inplace=True)

final_data = pd.concat([dataset, add_info_final])
final_data

sum(final_data['Documents'].isna())

from google.colab import files

# Save DataFrame to a CSV file
final_data.to_csv('final_data.csv', index=False)

# Download the file
files.download('final_data.csv')

"""## Data Exploration

### Word frequency analysis
"""

def preprocess_text(text):
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))

    # Tokenize
    tokens = word_tokenize(text)

    # Convert to lowercase
    tokens = [word.lower() for word in tokens]

    # Remove stop words
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]

    # Lemmatize
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    return tokens

#Plotting the most common words
processed_reviews = pd.DataFrame({'review_tokens': final_data['Documents'].apply(preprocess_text)})
all_tokens = [token for review in processed_reviews['review_tokens'] for token in review]
word_freq = Counter(all_tokens)
common_words = word_freq.most_common(25)

word_freq_df = pd.DataFrame(common_words, columns=['word', 'frequency'])
# plt.figure(figsize=(10, 6))
# sns.barplot(x='frequency', y='word', data=word_freq_df)
# plt.title('Most Common Words')
# plt.show()

# Plot customization
plt.figure(figsize=(10, 6))
sns.barplot(
    x='word',          # Words on the x-axis
    y='frequency',     # Frequency on the y-axis
    data=word_freq_df
)

# Adding labels and title
plt.title('Most Common Words in Reviews', fontsize=16)
plt.xlabel('Word', fontsize=14)
plt.ylabel('Frequency', fontsize=14)

# Rotate x-axis labels for better readability
plt.xticks(rotation=45)

# Show the plot
plt.tight_layout()
plt.show()

# Word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud')
plt.show()

"""### How many words per review"""

processed_reviews['words_count'] = final_data['Documents'].apply(lambda x: len(word_tokenize(x)))
plt.figure(figsize=(10, 6))
sns.histplot(processed_reviews['words_count'], binwidth=20, kde=True)
plt.title('Distribution of Review Lengths (in words)')
plt.xlabel('Number of Words')
plt.ylabel('Frequency')
plt.show()

"""### How many reviews for accomodation"""

grouped = data.groupby('url').size()

# Calculate statistics
average_reviews = grouped.mean()
min_reviews = grouped.min()
max_reviews = grouped.max()

# Display the results
print(f"Average number of reviews per accommodation: {average_reviews}")
print(f"Minimum number of reviews: {min_reviews}")
print(f"Maximum number of reviews: {max_reviews}")

"""### Zipf's and Heap's Law verifications"""

# Tokenize all reviews
all_words = ' '.join(data['EnglishReview']).split()

# Compute word frequencies
word_freq = Counter(all_words)

# Sort by frequency
ranked_freq = sorted(word_freq.values(), reverse=True)
ranks = np.arange(1, len(ranked_freq) + 1)

# Log-log plot
plt.figure(figsize=(8, 5))
plt.loglog(ranks, ranked_freq, marker='o', linestyle='none')
plt.title("Zipf's Law Verification")
plt.xlabel("Rank")
plt.ylabel("Frequency")
plt.show()

# Tokenize all reviews
all_words = []
vocab_sizes = []
total_words = []

# Compute vocabulary size iteratively
for review in data['EnglishReview']:
    words = review.split()
    all_words.extend(words)
    vocab_sizes.append(len(set(all_words)))
    total_words.append(len(all_words))

# Plot Heaps' Law
plt.figure(figsize=(8, 5))
plt.plot(total_words, vocab_sizes, label="Vocabulary Growth")
plt.title("Heaps' Law Verification")
plt.xlabel("Total Words (N)")
plt.ylabel("Vocabulary Size (V)")
plt.legend()
plt.show()

# Optional: Fit a curve to find k and beta
from scipy.optimize import curve_fit

def heaps_law(N, k, beta):
    return k * N ** beta

params, _ = curve_fit(heaps_law, total_words, vocab_sizes)
print(f"Estimated k: {params[0]}, Estimated beta: {params[1]}")

"""## RAG system

### Loading data
"""

final_data = pd.read_csv("final_data.csv")

from langchain.document_loaders import DataFrameLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import CacheBackedEmbeddings, HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.storage import LocalFileStore

#Dataframe loader
loader = DataFrameLoader(final_data, page_content_column = 'Documents')  #Documents is used as content, while all other columns are added as metadata
documents = loader.load()

"""### Text Splitter"""

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 1000,
    chunk_overlap = 100,
    length_function = len
)

splitted_documents = text_splitter.transform_documents(documents)
print("The number of final documents is: ", len(splitted_documents))

"""## Embedding model
In the following block, we performed a series of operations to prepare and optimize an embedding model for sentence similarity evaluation

We tested three different embedding models provided by Hugging Face:
- [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)
- [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)
- [bert-base-nli-mean-tokens](https://huggingface.co/sentence-transformers/bert-base-nli-mean-tokens)

Our choice fell on ```all-mpnet-base-v2```, as it was the one that brought the best results in the experiments we did.

The embedding model will be loaded with a cache to improve performance (it avoids re-computing embeddings for the same queries).

Finally, we will create an archive of vectors via [FAISS](https://github.com/facebookresearch/faiss/) on which we will perform similarity search.


"""

cache_path = LocalFileStore('./cache/')

# HuggingFace name of the desired model
model_id = 'sentence-transformers/all-MiniLM-L6-v2'

# Creation of the embedding given the selected model
embedding_model = HuggingFaceEmbeddings(model_name = model_id)

# Caching to avoid embedding the same queries multiple times
embedder = CacheBackedEmbeddings.from_bytes_store(embedding_model, cache_path, namespace = model_id)

# FAISS vector store of the reviews
vector_store = FAISS.from_documents(splitted_documents, embedder)

"""## Retrieval without metadata filtering"""

# here we perform a similarity search to get a glance of the performance of the embedder
query = 'I need to do a medical visit. Which apartment is closer to the hospital? I also need more rooms since I have children'

embedding_vector = embedding_model.embed_query(query)
docs = vector_store.similarity_search_by_vector(embedding_vector, k = 10)

for idx, doc in enumerate(docs):
  print(f'Document n. {idx+1}\nContent: {doc.page_content}\nMetadata: {doc.metadata}\n')

"""## Retrieval with metadata filtering"""

# The following code is adapted from the notebook by [Original Author Name]
# Repository: https://github.com/OriginalAuthor/OriginalRepo
# Notebook: https://github.com/OriginalAuthor/OriginalRepo/blob/main/notebook_name.ipynb

import json
from typing import Dict, List

from haystack import Pipeline, component
from haystack.components.builders import PromptBuilder
from haystack.components.generators import OpenAIGenerator

from haystack import Document
from haystack.document_stores.faiss import FAISS
from haystack.embeddings import HuggingFaceEmbeddings
from haystack.utils import LocalFileStore
from haystack.nodes import CacheBackedEmbeddings

@component()
class QueryMetadataExtractor:

    def __init__(self):
        prompt = """
        You are part of an information system that processes users queries.
        Given a user query you extract information from it that matches a given list of metadata fields.
        The information to be extracted from the query must match the semantics associated with the given metadata fields.
        The information that you extracted from the query will then be used as filters to narrow down the search space
        when querying an index.
        Just include the value of the extracted metadata without including the name of the metadata field.
        The extracted information in 'Extracted metadata' must be returned as a valid JSON structure.
        ###
        Example 1:
        Query: "I would like to stay in a room in a shared apartment with private bathroom in Padua. I also have a dog, so pets should be allowed.
        What is the best accomodation for me?"
        Metadata fields: {"roomType", "bathroomLabel","allowsPets"}
        Extracted metadata fields: {"roomType": "private room in apartment", "bathroomLabel": "1 private bathroom", "allowsPets": True}
        ###
        Example 2:
        Query: "I am planning a trip near Padua. We're in 4 and we would like to have 2 bedrooms, since there are two children. Some of us also smoke, so it would be better to
        have a place where is it possible to do so. What is the best apartment suggested?"
        Metadata fields: {"personCapacity", "bedroomLabel","allowChildren", "allowsSmoking"}
        Extracted metadata fields: {"personCapacity": "4", "bedroomLabel": "2 bedrooms", "allowChildren": True, "allowsSmoking": True}
        ###
        Example 3:
        Query: "We are a family with a toddler. We would like to stay in an entire apartment with 2 bathrooms in the centre of Padua.
        I prefer apartments hosted by a superhost since I think they are more relialable. We would like to invite some friends for Christmas and do a small party in the house.
        Which is the best solution to our needs?"
        Metadata fields: {"roomType", "bathroomLabel", "isHostedBySuperhost", "allowsInfant", "allowsEvent"}
        Extracted metadata fields: {"roomType": "Entire accommodation: apartment", "bathroomLabel": "2 bathrooms", "isHostedBySuperhost": True,
        "allowsInfant": True,"allowsEvent": True}
        ###
        Example 4:
        Query: "{{query}}"
        Metadata fields: "{{metadata_fields}}"
        Extracted metadata fields:
        """
        self.pipeline = Pipeline()
        self.pipeline.add_component(name="builder", instance=PromptBuilder(prompt))
        self.pipeline.add_component(name="llm", instance=OpenAIGenerator(model="gpt-4o-mini"))
        self.pipeline.connect("builder", "llm")

    @component.output_types(filters=Dict[str, str])
    def run(self, query: str, metadata_fields: List[str]):
        result = self.pipeline.run({'builder': {'query': query, 'metadata_fields': metadata_fields}})
        metadata = json.loads(result['llm']['replies'][0])

        filters = []
        for key, value in metadata.items():
            field = f"meta.{key}"
            filters.append({"field": field, "operator": "==", "value": value})

        return {"filters": {"operator": "AND", "conditions": filters}}

# Example metadata filtering function
def filter_documents_by_metadata(metadata_filters, documents):
    filtered_docs = []
    for doc in documents:
        match = True
        for key, value in metadata_filters.items():
            if key not in doc.meta or doc.meta[key] != value:
                match = False
                break
        if match:
            filtered_docs.append(doc)
    return filtered_docs

# Metadata fields extraction from query
query = "We need to stay in Padua for 3 days. We are a family of 5 people with 2 toddlers. We would like to have 2 bedrooms and we prefer to be close to the univerisity area in Portello. Which is the best accomodation for us?"
metadata_fields = {"personCapacity", "allowsInfant", "bedroomLabel"}

# Extract metadata from query
metadata_extractor = QueryMetadataExtractor()
metadata = metadata_extractor.run(query, list(metadata_fields))["filters"]["conditions"]
metadata_filters = {cond["field"].replace("meta.", ""): cond["value"] for cond in metadata}

# Now we filter the documents based on the extracted metadata
filtered_docs = filter_documents_by_metadata(metadata_filters, vector_store.get_all_documents())

# If there are any filtered documents, proceed to similarity search
if filtered_docs:
    embedding_vector = embedding_model.embed_query(query)
    filtered_vector_store = FAISS.from_documents(filtered_docs, embedder)
    docs = filtered_vector_store.similarity_search_by_vector(embedding_vector, k=10)

    for idx, doc in enumerate(docs):
        print(f'Document n. {idx+1}\nContent: {doc.page_content}\nMetadata: {doc.metadata}\n')
else:
    print("No documents found matching the metadata filters.")

"""### Text Splitter"""

from huggingface_hub import notebook_login
token = "insert_your_token"
notebook_login()

import torch
import transformers

model_id = "meta-llama/Llama-2-13b-chat-hf"

bnb_config = transformers.BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16
)

model_config = transformers.AutoConfig.from_pretrained(
    model_id
)

model = transformers.AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    config=model_config,
    quantization_config=bnb_config,
    device_map='auto'
)

model.eval()

tokenizer = transformers.AutoTokenizer.from_pretrained(
    model_id
)

generate_text = transformers.pipeline(
    model=model,
    tokenizer=tokenizer,
    task="text-generation",
    return_full_text=True,
    do_sample=True,
    temperature=0.2,
    max_new_tokens=256
)

from langchain.llms import HuggingFacePipeline

llm = HuggingFacePipeline(pipeline=generate_text)

from langchain_core.prompts.prompt import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.callbacks import StdOutCallbackHandler

# prompt
template = (
    """
    <s>[INST]
    You are an assistant for question-answering tasks.
    The theme of the questions is: AirBnB reviews and house rules in Padua and province.
    Use the following pieces of retrieved context to answer the question.
    Feel free to ignore some context if it is not useful for the answer.
    If you don't know the answer, apologize and say you don't know.
    If there isn't any accomodation which is perfect with the needs of the client, you should suggest some alternatives,
    highlighting what are the things which do not correspond to the requested needs.
    Please ensure your responses are supported by the information from the retrieved documents.

    </s>
    [INST]
    Context: {context}
    [/INST]
    Question: {question}
    Helpful Answer:
    """
)

# this document template allows us to pass metadata to the model
# otherwise only page_content is used
document_prompt = PromptTemplate(
    input_variables = ['page_content', 'name', 'city', 'bathroomLabel', 'bedroomLabel', 'allowChildren', 'allowEvents', 'allowsInfant',
                       'allowsSmoking','personCapacity','isHostedBySuperhost','roomType'],
    template = """
        name: {name};
        city: {city};
        number of bathrooms: {bathroomLabel}
        number of bedrooms: {bedroomLabel};
        allows children: {allowChildren};
        allows events: {allowsEvents};
        allows infant: {allowsInfant};
        allows pets: {allowsPets};
        allows smoking: {allowsSmoking};
        person capacity: {personCapacity};
        is hosted by a superhost: {isHostedBySuperhost};
        room type: {roomType};
        review or house rule: {page_content}"""
)

# retriever based on the vector store to perform sentence similarity searches
retriever = vector_store.as_retriever(search_type='similarity', search_kwargs = {"k" : 5})

# callback for std out
stdout_handler = StdOutCallbackHandler()

# QA chain
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type = "stuff",
    retriever = retriever,
    callbacks=[stdout_handler],
    chain_type_kwargs = {
        "prompt": PromptTemplate(template=template, input_variables=[]),
        "document_prompt": document_prompt
    },
    return_source_documents=True
)

qa({'query' : "I need to do a medical visit. Which apartment is closer to the hospital? I also need more rooms since I have children"})

